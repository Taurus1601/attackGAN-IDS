{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples, malicious_ratio=0.5):\n",
    "    num_malicious = int(num_samples * malicious_ratio)\n",
    "    num_normal = num_samples - num_malicious\n",
    "\n",
    "    # Normal logs (e.g., random normal distribution)\n",
    "    normal_logs = np.random.normal(0, 1, (num_normal, 20))\n",
    "\n",
    "    # Malicious logs (e.g., random distribution with distinct pattern)\n",
    "    malicious_logs = np.random.normal(3, 1, (num_malicious, 20))\n",
    "\n",
    "    return np.concatenate([normal_logs, malicious_logs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_synthetic_data(1000)\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))  # Normalize to [0, 1]\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Generator model\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(20,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(20, activation=\"tanh\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the Discriminator model\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_shape=(20,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training Step\n",
    "@tf.function\n",
    "def train_step(logs):\n",
    "    noise = tf.random.normal([BATCH_SIZE, 20])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_logs = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(logs, training=True)\n",
    "        fake_output = discriminator(generated_logs, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 10:18:55.245956: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-08 10:18:55.292065: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-08 10:18:55.400430: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 completed\n",
      "Epoch 2/50 completed\n",
      "Epoch 3/50 completed\n",
      "Epoch 4/50 completed\n",
      "Epoch 5/50 completed\n",
      "Epoch 6/50 completed\n",
      "Epoch 7/50 completed\n",
      "Epoch 8/50 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 10:18:55.658655: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 completed\n",
      "Epoch 10/50 completed\n",
      "Epoch 11/50 completed\n",
      "Epoch 12/50 completed\n",
      "Epoch 13/50 completed\n",
      "Epoch 14/50 completed\n",
      "Epoch 15/50 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 10:18:56.192344: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 completed\n",
      "Epoch 17/50 completed\n",
      "Epoch 18/50 completed\n",
      "Epoch 19/50 completed\n",
      "Epoch 20/50 completed\n",
      "Epoch 21/50 completed\n",
      "Epoch 22/50 completed\n",
      "Epoch 23/50 completed\n",
      "Epoch 24/50 completed\n",
      "Epoch 25/50 completed\n",
      "Epoch 26/50 completed\n",
      "Epoch 27/50 completed\n",
      "Epoch 28/50 completed\n",
      "Epoch 29/50 completed\n",
      "Epoch 30/50 completed\n",
      "Epoch 31/50 completed\n",
      "Epoch 32/50 completed\n",
      "Epoch 33/50 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 10:18:57.278910: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 completed\n",
      "Epoch 35/50 completed\n",
      "Epoch 36/50 completed\n",
      "Epoch 37/50 completed\n",
      "Epoch 38/50 completed\n",
      "Epoch 39/50 completed\n",
      "Epoch 40/50 completed\n",
      "Epoch 41/50 completed\n",
      "Epoch 42/50 completed\n",
      "Epoch 43/50 completed\n",
      "Epoch 44/50 completed\n",
      "Epoch 45/50 completed\n",
      "Epoch 46/50 completed\n",
      "Epoch 47/50 completed\n",
      "Epoch 48/50 completed\n",
      "Epoch 49/50 completed\n",
      "Epoch 50/50 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataset:\n",
    "            train_step(batch)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "\n",
    "# Train the GAN\n",
    "EPOCHS = 50\n",
    "train(train_dataset, EPOCHS)\n",
    "\n",
    "# Generate and visualize synthetic samples\n",
    "def generate_synthetic_samples(model, num_samples=16):\n",
    "    noise = tf.random.normal([num_samples, 20])\n",
    "    generated_logs = model(noise, training=False)\n",
    "    return generated_logs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic system logs:\n",
      "tf.Tensor(\n",
      "[[0.3224162  0.5213434  0.235065   0.3266033  0.40846312 0.461739\n",
      "  0.42588893 0.37299448 0.23601027 0.32338554 0.36779317 0.4648808\n",
      "  0.35508323 0.5197323  0.50312066 0.2666637  0.4419969  0.35572928\n",
      "  0.5092957  0.3551684 ]\n",
      " [0.40948376 0.6519361  0.39743674 0.46134278 0.53054416 0.655108\n",
      "  0.5465966  0.46716937 0.307567   0.63757515 0.34873894 0.5489998\n",
      "  0.49650902 0.5764584  0.4122931  0.41416126 0.39178455 0.38765472\n",
      "  0.58962244 0.4210595 ]\n",
      " [0.6077027  0.80051064 0.72317517 0.49104667 0.6319538  0.74686617\n",
      "  0.7442014  0.64444983 0.6175591  0.5995289  0.6662665  0.7998689\n",
      "  0.75394756 0.77295285 0.6170922  0.45069304 0.70270973 0.6540614\n",
      "  0.75992054 0.6145673 ]\n",
      " [0.2326051  0.57135195 0.23488228 0.27198443 0.4621282  0.4875439\n",
      "  0.4156847  0.31930712 0.32477504 0.37336707 0.34887755 0.4752807\n",
      "  0.43713868 0.49450946 0.4382023  0.2526059  0.47177035 0.36419016\n",
      "  0.55511534 0.39643946]\n",
      " [0.4197669  0.572843   0.37782133 0.3381527  0.514815   0.5876318\n",
      "  0.51518804 0.3327596  0.3673414  0.5224673  0.3636003  0.53267723\n",
      "  0.4731333  0.46914512 0.44856185 0.347006   0.34265262 0.36667755\n",
      "  0.5389383  0.3468175 ]], shape=(5, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate samples after training\n",
    "generated_samples = generate_synthetic_samples(generator, num_samples=5)\n",
    "print(\"Generated synthetic system logs:\")\n",
    "print(generated_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack-gan model from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence      label\n",
      "0  ['168', '168', '168', '168', '168', '168', '26...  malicious\n",
      "1  ['6', '33', '6', '268', '6', '6', '5', '102', ...  malicious\n",
      "2  ['168', '265', '3', '3', '265', '265', '168', ...  malicious\n",
      "3  ['311', '240', '240', '6', '6', '6', '6', '6',...     benign\n",
      "4  ['197', '125', '20', '45', '5', '42', '120', '...     benign\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'adfa_ld_processed.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/b3/2cyr9kb15bx6073klxmmzym00000gn/T/ipykernel_95014/2247111671.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cleaned_row = re.findall(r'\\d+', row[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re  # To clean and extract integers from the string\n",
    "\n",
    "# Load your dataset\n",
    "data_path = 'adfa_ld_processed.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Cleaning and converting each row to a list of integers\n",
    "def clean_and_convert(row):\n",
    "    # Remove any non-numeric characters (like brackets or quotes)\n",
    "    cleaned_row = re.findall(r'\\d+', row[0])\n",
    "    # Convert to integers\n",
    "    return list(map(int, cleaned_row))\n",
    "\n",
    "# Apply the cleaning and conversion function to each row\n",
    "data_sequences = data.apply(clean_and_convert, axis=1)\n",
    "\n",
    "# Step 2: Determine a target length (e.g., median length)\n",
    "sequence_lengths = data_sequences.apply(len)\n",
    "target_length = int(sequence_lengths.median())\n",
    "\n",
    "# Step 3: Pad sequences to the target length\n",
    "padded_sequences = pad_sequences(data_sequences, maxlen=target_length, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "# Step 4: Normalize the padded sequences\n",
    "scaler = MinMaxScaler()\n",
    "padded_sequences_normalized = scaler.fit_transform(padded_sequences.reshape(-1, target_length))\n",
    "\n",
    "# Reshape back to original format if needed\n",
    "padded_sequences_normalized = padded_sequences_normalized.reshape(-1, target_length)\n",
    "\n",
    "# Now `padded_sequences_normalized` is ready for use in the GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5357143 , 0.5030488 , 0.48961425, ..., 0.49411765, 0.7794118 ,\n",
       "        0.5436893 ],\n",
       "       [0.00974026, 0.09146341, 0.00890208, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.5357143 , 0.79878044, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.00974026, 0.6646341 , 0.64688426, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.28571427, 0.5762195 , 0.00890208, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.7694805 , 0.72256094, 0.08902077, ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set some parameters\n",
    "INPUT_SHAPE = padded_sequences_normalized.shape[1]  # Assuming fixed length after padding\n",
    "LATENT_DIM = 100  # Dimension of the random noise vector input to the generator\n",
    "\n",
    "# Define the Generator Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(INPUT_SHAPE, activation=\"tanh\"))  # Output shape matches log sequence length\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the Discriminator Model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_shape=(INPUT_SHAPE,)))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))  # Binary classification: benign (0) or malicious (1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the generator and discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Define Loss Functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # Goal: Fool the discriminator into classifying fake logs as malicious\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # '1' for malicious\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # Discriminator wants to correctly classify benign logs as 0 and malicious logs as 1\n",
    "    real_loss = cross_entropy(tf.zeros_like(real_output), real_output)  # '0' for benign\n",
    "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)  # '1' for malicious\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# Define Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(5e-5)  # Lower the learning rate\n",
    "\n",
    "\n",
    "\n",
    "# Training Step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(real_logs):\n",
    "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_logs = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(real_logs, training=True)\n",
    "        fake_output = discriminator(generated_logs, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training Loop\n",
    "def train_gan(dataset, epochs):\n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gen_loss_epoch = 0.0\n",
    "        disc_loss_epoch = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(batch)\n",
    "            gen_loss_epoch += gen_loss\n",
    "            disc_loss_epoch += disc_loss\n",
    "            batch_count += 1\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        generator_losses.append(gen_loss_epoch / batch_count)\n",
    "        discriminator_losses.append(disc_loss_epoch / batch_count)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Generator Loss: {gen_loss_epoch / batch_count:.4f}, Discriminator Loss: {disc_loss_epoch / batch_count:.4f}\")\n",
    "\n",
    "    # Plotting the losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(generator_losses, label=\"Generator Loss\")\n",
    "    plt.plot(discriminator_losses, label=\"Discriminator Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Generator and Discriminator Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare Dataset\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Convert normalized sequences to TensorFlow dataset and batch them\n",
    "malicious_dataset = tf.data.Dataset.from_tensor_slices(padded_sequences_normalized).shuffle(len(padded_sequences_normalized)).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:21.051032: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-11 11:36:21.220654: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - Generator Loss: 0.2097, Discriminator Loss: 0.5700\n",
      "Epoch 2/1000 - Generator Loss: 0.0221, Discriminator Loss: 0.1559\n",
      "Epoch 3/1000 - Generator Loss: 0.0070, Discriminator Loss: 0.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:21.788656: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000 - Generator Loss: 0.0035, Discriminator Loss: 0.0519\n",
      "Epoch 5/1000 - Generator Loss: 0.0020, Discriminator Loss: 0.0385\n",
      "Epoch 6/1000 - Generator Loss: 0.0013, Discriminator Loss: 0.0299\n",
      "Epoch 7/1000 - Generator Loss: 0.0009, Discriminator Loss: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:22.908112: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000 - Generator Loss: 0.0006, Discriminator Loss: 0.0221\n",
      "Epoch 9/1000 - Generator Loss: 0.0005, Discriminator Loss: 0.0202\n",
      "Epoch 10/1000 - Generator Loss: 0.0004, Discriminator Loss: 0.0169\n",
      "Epoch 11/1000 - Generator Loss: 0.0003, Discriminator Loss: 0.0157\n",
      "Epoch 12/1000 - Generator Loss: 0.0003, Discriminator Loss: 0.0143\n",
      "Epoch 13/1000 - Generator Loss: 0.0002, Discriminator Loss: 0.0133\n",
      "Epoch 14/1000 - Generator Loss: 0.0002, Discriminator Loss: 0.0124\n",
      "Epoch 15/1000 - Generator Loss: 0.0002, Discriminator Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:24.516383: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000 - Generator Loss: 0.0002, Discriminator Loss: 0.0106\n",
      "Epoch 17/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0104\n",
      "Epoch 18/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0093\n",
      "Epoch 19/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0091\n",
      "Epoch 20/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0089\n",
      "Epoch 21/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0087\n",
      "Epoch 22/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0077\n",
      "Epoch 23/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0073\n",
      "Epoch 24/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0069\n",
      "Epoch 25/1000 - Generator Loss: 0.0001, Discriminator Loss: 0.0069\n",
      "Epoch 26/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0066\n",
      "Epoch 27/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0060\n",
      "Epoch 28/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0057\n",
      "Epoch 29/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0055\n",
      "Epoch 30/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0051\n",
      "Epoch 31/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:27.736373: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0050\n",
      "Epoch 33/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0046\n",
      "Epoch 34/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0043\n",
      "Epoch 35/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0043\n",
      "Epoch 36/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0043\n",
      "Epoch 37/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0037\n",
      "Epoch 38/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0037\n",
      "Epoch 39/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0032\n",
      "Epoch 40/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0033\n",
      "Epoch 41/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0034\n",
      "Epoch 42/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0031\n",
      "Epoch 43/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0029\n",
      "Epoch 44/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0028\n",
      "Epoch 45/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0026\n",
      "Epoch 46/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0024\n",
      "Epoch 47/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0025\n",
      "Epoch 48/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0022\n",
      "Epoch 49/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0021\n",
      "Epoch 50/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0019\n",
      "Epoch 51/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0018\n",
      "Epoch 52/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0019\n",
      "Epoch 53/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0017\n",
      "Epoch 54/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0016\n",
      "Epoch 55/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0015\n",
      "Epoch 56/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0015\n",
      "Epoch 57/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0013\n",
      "Epoch 58/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0014\n",
      "Epoch 59/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0014\n",
      "Epoch 60/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0013\n",
      "Epoch 61/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0012\n",
      "Epoch 62/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0011\n",
      "Epoch 63/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:34.262748: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0011\n",
      "Epoch 65/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0010\n",
      "Epoch 66/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0010\n",
      "Epoch 67/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0009\n",
      "Epoch 68/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0008\n",
      "Epoch 69/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0008\n",
      "Epoch 70/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0007\n",
      "Epoch 71/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0007\n",
      "Epoch 72/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0007\n",
      "Epoch 73/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0006\n",
      "Epoch 74/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0007\n",
      "Epoch 75/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0006\n",
      "Epoch 76/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 77/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 78/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 79/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 80/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 81/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 82/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0005\n",
      "Epoch 83/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 84/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 85/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 86/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 87/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 88/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 89/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0004\n",
      "Epoch 90/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 91/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 92/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 93/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 94/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 95/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 96/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 97/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 98/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 99/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0003\n",
      "Epoch 100/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 101/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 102/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 103/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 104/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 105/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 106/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 107/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 108/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 109/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 110/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 111/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0002\n",
      "Epoch 112/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 113/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 114/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 115/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 116/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 117/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 118/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 119/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 120/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 121/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 122/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 123/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 124/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 125/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 126/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 127/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 128/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:36:47.259122: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 130/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 131/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 132/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 133/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 134/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 135/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 136/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 137/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 138/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 139/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 140/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 141/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 142/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 143/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0001\n",
      "Epoch 144/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 145/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 146/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 147/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 148/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 149/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 150/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 151/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 152/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 153/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 154/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 155/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 156/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 157/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 158/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 159/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 160/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 161/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 162/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 163/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 164/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 165/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 166/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 167/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 168/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 169/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 170/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 171/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 172/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 173/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 174/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 175/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 176/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 177/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 178/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 179/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 180/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 181/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 182/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 183/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 184/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 185/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 186/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 187/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 188/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 189/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 190/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 191/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 192/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 193/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 194/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 195/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 196/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 197/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 198/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 199/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 200/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 201/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 202/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 203/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 204/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 205/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 206/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 207/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 208/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 209/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 210/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 211/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 212/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 213/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 214/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 215/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 216/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 217/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 218/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 219/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 220/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 221/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 222/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 223/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 224/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 225/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 226/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 227/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 228/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 229/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 230/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 231/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 232/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 233/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 234/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 235/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 236/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 237/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 238/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 239/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 240/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 241/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 242/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 243/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 244/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 245/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 246/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 247/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 248/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 249/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 250/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 251/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 252/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 253/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 254/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 255/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:37:14.058161: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 257/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 258/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 259/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 260/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 261/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 262/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 263/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 264/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 265/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 266/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 267/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 268/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 269/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 270/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 271/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 272/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 273/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 274/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 275/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 276/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 277/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 278/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 279/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 280/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 281/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 282/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 283/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 284/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 285/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 286/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 287/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 288/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 289/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 290/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 291/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 292/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 293/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 294/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 295/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 296/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 297/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 298/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 299/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 300/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 301/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 302/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 303/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 304/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 305/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 306/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 307/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 308/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 309/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 310/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 311/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 312/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 313/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 314/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 315/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 316/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 317/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 318/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 319/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 320/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 321/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 322/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 323/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 324/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 325/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 326/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 327/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 328/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 329/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 330/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 331/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 332/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 333/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n",
      "Epoch 334/1000 - Generator Loss: 0.0000, Discriminator Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmalicious_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 33\u001b[0m     gen_loss, disc_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     gen_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gen_loss\n\u001b[1;32m     35\u001b[0m     disc_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m disc_loss\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the GAN\n",
    "train_gan(malicious_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate and display synthetic malicious samples\n",
    "def generate_synthetic_logs(model, num_samples=5):\n",
    "    noise = tf.random.normal([num_samples, LATENT_DIM])\n",
    "    generated_logs = model(noise, training=False)\n",
    "    return generated_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic malicious logs:\n",
      "tf.Tensor(\n",
      "[[-0.9139534   0.9169651  -0.54012746 ... -0.7829064   0.6967921\n",
      "   0.06413009]\n",
      " [-0.46344137  0.8475406  -0.7666232  ... -0.8594527   0.27555704\n",
      "   0.19389987]\n",
      " [-0.7231584   0.72466826 -0.5602551  ... -0.7690965   0.6210189\n",
      "   0.0547808 ]\n",
      " [-0.6252184   0.81358796 -0.39786765 ... -0.66503036  0.62949437\n",
      "  -0.27921933]\n",
      " [-0.66082126  0.88432515 -0.4751598  ... -0.7783639   0.12432697\n",
      "   0.06970598]], shape=(5, 229), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display a few generated samples\n",
    "generated_samples = generate_synthetic_logs(generator, num_samples=5)\n",
    "print(\"Generated synthetic malicious logs:\")\n",
    "print(generated_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Accuracy on Real vs Fake Logs: 0.0\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_discriminator_accuracy(real_logs, fake_logs):\n",
    "    # Get predictions for real and fake logs\n",
    "    real_predictions = discriminator(real_logs, training=False)\n",
    "    fake_predictions = discriminator(fake_logs, training=False)\n",
    "    \n",
    "    # Threshold to classify: values > 0.5 are \"real,\" values <= 0.5 are \"fake\"\n",
    "    real_accuracy = tf.reduce_mean(tf.cast(real_predictions > 0.5, tf.float32))\n",
    "    fake_accuracy = tf.reduce_mean(tf.cast(fake_predictions <= 0.5, tf.float32))\n",
    "    \n",
    "    # Total accuracy is the mean of both real and fake accuracies\n",
    "    discriminator_accuracy = (real_accuracy + fake_accuracy) / 2\n",
    "    return discriminator_accuracy\n",
    "\n",
    "# Example usage within training:\n",
    "real_logs_sample = next(iter(malicious_dataset))  # Get a batch of real logs\n",
    "noise_sample = tf.random.normal([BATCH_SIZE, LATENT_DIM])  # Generate noise\n",
    "fake_logs_sample = generator(noise_sample, training=False)  # Generate fake logs\n",
    "disc_accuracy = compute_discriminator_accuracy(real_logs_sample, fake_logs_sample)\n",
    "print(\"Discriminator Accuracy on Real vs Fake Logs:\", disc_accuracy.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator's Success Rate (Fooling Rate): 1.0\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_generator_success_rate(fake_logs):\n",
    "    # Generate fake logs and get predictions from the discriminator\n",
    "    predictions = discriminator(fake_logs, training=False)\n",
    "    # Success rate: how often the fake logs are classified as \"real\" by the discriminator\n",
    "    success_rate = tf.reduce_mean(tf.cast(predictions > 0.5, tf.float32))\n",
    "    return success_rate\n",
    "\n",
    "# Example usage:\n",
    "fake_logs_sample = generator(noise_sample, training=False)  # Generate fake logs\n",
    "gen_success_rate = compute_generator_success_rate(fake_logs_sample)\n",
    "print(\"Generator's Success Rate (Fooling Rate):\", gen_success_rate.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data saved to generated_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generated_df = pd.DataFrame(generated_samples.numpy() if hasattr(generated_samples, 'numpy') else generated_samples)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'generated_data.csv'  # Change this path as needed\n",
    "generated_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Generated data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denormalized integer data saved to denormalized_data_integers.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming `scaler` is the MinMaxScaler instance used for normalization\n",
    "# and `padded_sequences_normalized` is the normalized data.\n",
    "\n",
    "# Denormalizing function\n",
    "def denormalize_data_to_integers(normalized_data, scaler, target_length):\n",
    "    \"\"\"\n",
    "    Denormalizes the normalized data using the scaler and converts to integers.\n",
    "    \n",
    "    Parameters:\n",
    "    - normalized_data: np.array, normalized data that needs to be denormalized\n",
    "    - scaler: MinMaxScaler instance, the scaler used to normalize the original data\n",
    "    - target_length: int, length to reshape the denormalized data if necessary\n",
    "    \n",
    "    Returns:\n",
    "    - denormalized_data: np.array, data in original scale as integers\n",
    "    \"\"\"\n",
    "    # Flatten the normalized data, denormalize it, and reshape back\n",
    "    denormalized_data = scaler.inverse_transform(normalized_data.reshape(-1, target_length))\n",
    "    # Convert to integers\n",
    "    denormalized_data = denormalized_data.round().astype(int)\n",
    "    return denormalized_data\n",
    "\n",
    "# Use the function to denormalize the data\n",
    "denormalized_data = denormalize_data_to_integers(padded_sequences_normalized, scaler, target_length)\n",
    "\n",
    "# Optionally, convert to DataFrame for easier viewing or saving\n",
    "denormalized_df = pd.DataFrame(denormalized_data)\n",
    "\n",
    "# Save the denormalized data to a CSV file\n",
    "output_path = 'denormalized_data_integers.csv'  # Path to save the CSV file\n",
    "denormalized_df.to_csv(output_path, index=False)\n",
    "print(f\"Denormalized integer data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the discriminator__ model___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/sanjanathyady/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m generated_seqs \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[1;32m     74\u001b[0m generated_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((half_batch, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 76\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(generated_seqs, generated_y)\n\u001b[1;32m     78\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:549\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 549\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    125\u001b[0m     outputs,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:61\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     53\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m     55\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     56\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[1;32m     62\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "# Python code to train a GAN for generating malicious syscall sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('ADFA_LD_PROCESSED.CSV')\n",
    "sequences = data['sequence'].apply(lambda x: eval(x)).tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# Encode syscalls\n",
    "all_syscalls = sorted(set([item for sublist in sequences for item in sublist]))\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(all_syscalls)\n",
    "encoded_sequences = [encoder.transform(seq) for seq in sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max(len(seq) for seq in encoded_sequences)\n",
    "X = pad_sequences(encoded_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define GAN components\n",
    "latent_dim = 100\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(256, input_dim=latent_dim),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(max_length, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(256, input_dim=max_length),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Compile GAN\n",
    "generator = build_generator()\n",
    "z = layers.Input(shape=(latent_dim,))\n",
    "generated_seq = generator(z)\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(generated_seq)\n",
    "gan = tf.keras.Model(z, valid)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "half_batch = batch_size // 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Train Discriminator\n",
    "    idx = np.random.randint(0, X.shape[0], half_batch)\n",
    "    real_seqs = X[idx]\n",
    "    real_y = np.ones((half_batch, 1))\n",
    "\n",
    "    noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "    generated_seqs = generator.predict(noise)\n",
    "    generated_y = np.zeros((half_batch, 1))\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(real_seqs, real_y)\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_seqs, generated_y)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    valid_y = np.ones((batch_size, 1))\n",
    "    g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch} / D Loss: {d_loss[0]} / D Acc: {100*d_loss[1]:.2f}% / G Loss: {g_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
