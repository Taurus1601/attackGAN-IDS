{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data\n",
    "Import required libraries (numpy, pandas, torch) and load the dataset. Handle missing values and convert categorical variables if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1579, 1, 2948])\n",
      "Value range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load and convert sequences\n",
    "data = pd.read_csv('adfa_ld_processed.csv')\n",
    "data['sequence'] = data['sequence'].apply(literal_eval)\n",
    "\n",
    "# Find max length and pad\n",
    "max_len = max(len(seq) for seq in data['sequence'])\n",
    "\n",
    "def pad_sequence(seq):\n",
    "    return np.pad(seq, (0, max_len - len(seq)), 'constant', constant_values=0)\n",
    "\n",
    "# Convert to numeric array\n",
    "X = np.array([pad_sequence([int(x) for x in seq]) for seq in data['sequence']])\n",
    "\n",
    "# Normalize to [0,1]\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Convert labels \n",
    "y = pd.get_dummies(data['label']).values\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "# Add channel dimension for CNN-GAN\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "\n",
    "print(f\"Data shape: {X_tensor.shape}\")\n",
    "print(f\"Value range: [{X_tensor.min():.3f}, {X_tensor.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4941, 0.4941, 0.4941,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0176, 0.0971, 0.0176,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.4941, 0.7794, 0.0088,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0176, 0.6500, 0.6500,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2676, 0.5647, 0.0176,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.7059, 0.7059, 0.0971,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Features to [-1, 1]\n",
    "Use MinMaxScaler or custom normalization to scale numerical features to the range [-1, 1], which is optimal for GAN training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the scaler to the data and transform it\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdata_array\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert the scaled data to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m     11\u001b[0m data_tensor_scaled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_array' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tensor Format\n",
    "Convert the normalized numpy arrays to PyTorch tensors with appropriate data types and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tensor shape: torch.Size([1579, 1, 2948])\n",
      "y_tensor shape: torch.Size([1579, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "# Add channel dimension for CNN-GAN\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "\n",
    "print(f\"X_tensor shape: {X_tensor.shape}\")\n",
    "print(f\"y_tensor shape: {y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tensor shape: torch.Size([1579, 1, 2948])\n",
      "Number of malicious samples: 746\n"
     ]
    }
   ],
   "source": [
    "# Check actual data dimensions\n",
    "print(f\"X_tensor shape: {X_tensor.shape}\")\n",
    "print(f\"Number of malicious samples: {(y_tensor[:, 1] == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6676\n",
      "Sample output: [0.40641993 0.51213396 0.26225832 0.7073802  0.49365905 0.512072\n",
      " 0.5425186  0.65626186 0.7273992  0.46501726]\n",
      "\n",
      "\n",
      "Epoch 10 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6670\n",
      "Sample output: [0.46024296 0.3321407  0.24131423 0.42760295 0.46592218 0.49297282\n",
      " 0.39785013 0.63778764 0.61452615 0.48506957]\n",
      "\n",
      "\n",
      "Epoch 20 Summary:\n",
      "Average D loss: 0.7021\n",
      "Average G loss: 0.6662\n",
      "Sample output: [0.29485673 0.30687013 0.22003867 0.4762129  0.4585861  0.49001122\n",
      " 0.47168234 0.64999634 0.6198393  0.32966527]\n",
      "\n",
      "\n",
      "Epoch 30 Summary:\n",
      "Average D loss: 0.7013\n",
      "Average G loss: 0.6651\n",
      "Sample output: [0.30894536 0.31976867 0.21658246 0.68099475 0.60940033 0.5584117\n",
      " 0.49394962 0.57751983 0.46970698 0.39768016]\n",
      "\n",
      "\n",
      "Epoch 40 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6650\n",
      "Sample output: [0.45597905 0.51905435 0.34841186 0.48495027 0.40114132 0.37034667\n",
      " 0.41007972 0.6965505  0.50605434 0.4538072 ]\n",
      "\n",
      "\n",
      "Epoch 50 Summary:\n",
      "Average D loss: 0.7006\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.4406785  0.44506282 0.38872823 0.63166136 0.51051193 0.6215192\n",
      " 0.46294078 0.58769655 0.5477708  0.5666552 ]\n",
      "\n",
      "\n",
      "Epoch 60 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6663\n",
      "Sample output: [0.34082064 0.30184835 0.3569951  0.5712052  0.4617608  0.32130706\n",
      " 0.49351487 0.41765994 0.668013   0.35763994]\n",
      "\n",
      "\n",
      "Epoch 70 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6645\n",
      "Sample output: [0.43226594 0.61433417 0.30133468 0.55551183 0.518614   0.5410792\n",
      " 0.71410775 0.42924657 0.4984524  0.36051545]\n",
      "\n",
      "\n",
      "Epoch 80 Summary:\n",
      "Average D loss: 0.7018\n",
      "Average G loss: 0.6661\n",
      "Sample output: [0.41922712 0.588691   0.2913673  0.5109231  0.5726726  0.53651804\n",
      " 0.6140397  0.5195418  0.5825145  0.52148056]\n",
      "\n",
      "\n",
      "Epoch 90 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6652\n",
      "Sample output: [0.47934356 0.4579198  0.29357764 0.56473607 0.5405659  0.2682995\n",
      " 0.42194596 0.47651044 0.5623712  0.28936312]\n",
      "\n",
      "\n",
      "Epoch 100 Summary:\n",
      "Average D loss: 0.7004\n",
      "Average G loss: 0.6662\n",
      "Sample output: [0.25975218 0.5529395  0.16850267 0.54367965 0.5396609  0.53935856\n",
      " 0.6329917  0.5073392  0.4508278  0.52248615]\n",
      "\n",
      "\n",
      "Epoch 110 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6674\n",
      "Sample output: [0.40775958 0.48310316 0.35141766 0.5187644  0.39292347 0.39259312\n",
      " 0.45843115 0.59611017 0.6728556  0.5010151 ]\n",
      "\n",
      "\n",
      "Epoch 120 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6655\n",
      "Sample output: [0.331824   0.42189837 0.23413494 0.5039278  0.59582996 0.49509463\n",
      " 0.42092308 0.6152299  0.6556083  0.38543206]\n",
      "\n",
      "\n",
      "Epoch 130 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6653\n",
      "Sample output: [0.24765974 0.3630479  0.24647272 0.4200075  0.5604945  0.38537154\n",
      " 0.44537643 0.52460134 0.5306809  0.6183281 ]\n",
      "\n",
      "\n",
      "Epoch 140 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6652\n",
      "Sample output: [0.48002622 0.52005625 0.2747289  0.639183   0.728383   0.3898853\n",
      " 0.4847806  0.6121308  0.42001018 0.5096535 ]\n",
      "\n",
      "\n",
      "Epoch 150 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6664\n",
      "Sample output: [0.36524475 0.2424745  0.4042708  0.54241335 0.51806974 0.41295835\n",
      " 0.5425584  0.5804881  0.47041503 0.3909734 ]\n",
      "\n",
      "\n",
      "Epoch 160 Summary:\n",
      "Average D loss: 0.7022\n",
      "Average G loss: 0.6669\n",
      "Sample output: [0.29672536 0.44027993 0.44786614 0.59507143 0.48027202 0.56362885\n",
      " 0.46922693 0.58864754 0.6021692  0.48594236]\n",
      "\n",
      "\n",
      "Epoch 170 Summary:\n",
      "Average D loss: 0.7013\n",
      "Average G loss: 0.6679\n",
      "Sample output: [0.35540536 0.3043987  0.3528265  0.6235507  0.46606216 0.37787873\n",
      " 0.4795983  0.6350578  0.57808137 0.5479695 ]\n",
      "\n",
      "\n",
      "Epoch 180 Summary:\n",
      "Average D loss: 0.7008\n",
      "Average G loss: 0.6669\n",
      "Sample output: [0.24938168 0.46639985 0.29307047 0.4343816  0.5871456  0.64256614\n",
      " 0.5658794  0.6265616  0.5465481  0.741929  ]\n",
      "\n",
      "\n",
      "Epoch 190 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.27462888 0.357309   0.30261764 0.5655067  0.721344   0.27922627\n",
      " 0.5220328  0.5234594  0.5556385  0.4024277 ]\n",
      "\n",
      "\n",
      "Epoch 200 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6642\n",
      "Sample output: [0.40741128 0.4084202  0.44526193 0.5031998  0.48784596 0.6239442\n",
      " 0.59744084 0.4765624  0.49647614 0.49293754]\n",
      "\n",
      "\n",
      "Epoch 210 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6658\n",
      "Sample output: [0.3313142  0.434936   0.22882667 0.62560004 0.5870113  0.6853581\n",
      " 0.51302034 0.5484188  0.4595056  0.5197075 ]\n",
      "\n",
      "\n",
      "Epoch 220 Summary:\n",
      "Average D loss: 0.7008\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.33801278 0.4732724  0.25425074 0.5513081  0.792782   0.42450097\n",
      " 0.4319429  0.44021285 0.5754676  0.5249348 ]\n",
      "\n",
      "\n",
      "Epoch 230 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6645\n",
      "Sample output: [0.34518442 0.41082686 0.41862455 0.49070695 0.5722298  0.6453354\n",
      " 0.5095353  0.57786393 0.49185356 0.46299988]\n",
      "\n",
      "\n",
      "Epoch 240 Summary:\n",
      "Average D loss: 0.7013\n",
      "Average G loss: 0.6669\n",
      "Sample output: [0.44985408 0.58333445 0.28130543 0.588271   0.50053906 0.56538904\n",
      " 0.5855602  0.50483143 0.4879247  0.63153297]\n",
      "\n",
      "\n",
      "Epoch 250 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6672\n",
      "Sample output: [0.40371028 0.53077996 0.332374   0.6477905  0.4875778  0.7342832\n",
      " 0.58643883 0.5614236  0.61117935 0.56712204]\n",
      "\n",
      "\n",
      "Epoch 260 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6653\n",
      "Sample output: [0.32083306 0.566545   0.37717876 0.63964826 0.5261662  0.6421896\n",
      " 0.568595   0.67614096 0.5274165  0.40322393]\n",
      "\n",
      "\n",
      "Epoch 270 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6656\n",
      "Sample output: [0.43819222 0.51378274 0.383523   0.5133142  0.6384857  0.49296588\n",
      " 0.46157527 0.55110806 0.5656814  0.46536422]\n",
      "\n",
      "\n",
      "Epoch 280 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6640\n",
      "Sample output: [0.36570778 0.42695954 0.29856017 0.4779954  0.42807773 0.53636295\n",
      " 0.54139745 0.40538436 0.54775095 0.5712909 ]\n",
      "\n",
      "\n",
      "Epoch 290 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6645\n",
      "Sample output: [0.37862968 0.5592913  0.30569285 0.5475502  0.513219   0.54332024\n",
      " 0.3867431  0.6119211  0.57841575 0.6494974 ]\n",
      "\n",
      "\n",
      "Epoch 300 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6661\n",
      "Sample output: [0.3655573  0.56288594 0.31076565 0.5964581  0.6382574  0.5899464\n",
      " 0.48734415 0.5167211  0.54136425 0.5836621 ]\n",
      "\n",
      "\n",
      "Epoch 310 Summary:\n",
      "Average D loss: 0.7018\n",
      "Average G loss: 0.6665\n",
      "Sample output: [0.2929384  0.34783325 0.22390163 0.46963125 0.5593882  0.44062337\n",
      " 0.4958099  0.6322842  0.6347662  0.49419352]\n",
      "\n",
      "\n",
      "Epoch 320 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6648\n",
      "Sample output: [0.36982158 0.5678638  0.27928564 0.56272113 0.61659086 0.56616104\n",
      " 0.4304711  0.45424506 0.5818909  0.47883967]\n",
      "\n",
      "\n",
      "Epoch 330 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6662\n",
      "Sample output: [0.3891506  0.4971341  0.38388586 0.53641236 0.57395875 0.58014333\n",
      " 0.5002347  0.6043968  0.42141688 0.45218447]\n",
      "\n",
      "\n",
      "Epoch 340 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6654\n",
      "Sample output: [0.5135423  0.6981246  0.32132733 0.5961155  0.6669736  0.48210636\n",
      " 0.4943885  0.52702165 0.37020576 0.5007045 ]\n",
      "\n",
      "\n",
      "Epoch 350 Summary:\n",
      "Average D loss: 0.7006\n",
      "Average G loss: 0.6672\n",
      "Sample output: [0.34181586 0.35342625 0.2867659  0.6078651  0.56111175 0.43018645\n",
      " 0.4549407  0.58028245 0.42844698 0.45663053]\n",
      "\n",
      "\n",
      "Epoch 360 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6675\n",
      "Sample output: [0.42590368 0.38225475 0.41303557 0.5140277  0.5072719  0.4232647\n",
      " 0.59055364 0.45791104 0.51067954 0.37612996]\n",
      "\n",
      "\n",
      "Epoch 370 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6657\n",
      "Sample output: [0.21684365 0.4063788  0.30588964 0.587702   0.39458612 0.5671012\n",
      " 0.5174514  0.4491183  0.5163073  0.3661282 ]\n",
      "\n",
      "\n",
      "Epoch 380 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.35380957 0.4908851  0.32499528 0.49549836 0.5291248  0.5463365\n",
      " 0.40570775 0.52688146 0.53431666 0.43521383]\n",
      "\n",
      "\n",
      "Epoch 390 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6653\n",
      "Sample output: [0.3231219  0.50541013 0.44182858 0.68117774 0.45566586 0.57164943\n",
      " 0.61857426 0.52076375 0.40736735 0.4567572 ]\n",
      "\n",
      "\n",
      "Epoch 400 Summary:\n",
      "Average D loss: 0.7021\n",
      "Average G loss: 0.6658\n",
      "Sample output: [0.30063793 0.53498197 0.36503866 0.6810667  0.5755339  0.61728865\n",
      " 0.32957616 0.61404306 0.5325537  0.5717905 ]\n",
      "\n",
      "\n",
      "Epoch 410 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6661\n",
      "Sample output: [0.3260602  0.4188522  0.42430654 0.5196107  0.40987644 0.49500906\n",
      " 0.5235946  0.7399527  0.46019503 0.43302265]\n",
      "\n",
      "\n",
      "Epoch 420 Summary:\n",
      "Average D loss: 0.7009\n",
      "Average G loss: 0.6657\n",
      "Sample output: [0.35085005 0.32246953 0.2168849  0.49512303 0.3591149  0.37193483\n",
      " 0.5370175  0.46638766 0.5158033  0.45445842]\n",
      "\n",
      "\n",
      "Epoch 430 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6667\n",
      "Sample output: [0.27749762 0.5520108  0.23867483 0.49561167 0.5780282  0.46110192\n",
      " 0.6140742  0.5880212  0.630352   0.51729894]\n",
      "\n",
      "\n",
      "Epoch 440 Summary:\n",
      "Average D loss: 0.7013\n",
      "Average G loss: 0.6649\n",
      "Sample output: [0.46022558 0.5615667  0.38607997 0.55533075 0.4698561  0.33576182\n",
      " 0.41133678 0.61802274 0.5698569  0.5167682 ]\n",
      "\n",
      "\n",
      "Epoch 450 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6677\n",
      "Sample output: [0.3753448  0.3729403  0.31939548 0.46445206 0.48453617 0.5726985\n",
      " 0.5861347  0.6937574  0.42309645 0.49171   ]\n",
      "\n",
      "\n",
      "Epoch 460 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6650\n",
      "Sample output: [0.3702859  0.3409432  0.29341218 0.44308868 0.5915653  0.40931988\n",
      " 0.37039283 0.49199244 0.5907187  0.40095565]\n",
      "\n",
      "\n",
      "Epoch 470 Summary:\n",
      "Average D loss: 0.7024\n",
      "Average G loss: 0.6657\n",
      "Sample output: [0.4013781  0.48959643 0.37895235 0.5151453  0.47243175 0.4749918\n",
      " 0.49013537 0.62349164 0.46659353 0.5707394 ]\n",
      "\n",
      "\n",
      "Epoch 480 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.44739318 0.6165929  0.4642857  0.60631925 0.5001705  0.40014616\n",
      " 0.50187975 0.6656531  0.58211285 0.43551263]\n",
      "\n",
      "\n",
      "Epoch 490 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6656\n",
      "Sample output: [0.50938374 0.3369464  0.262856   0.6288188  0.5184965  0.41363153\n",
      " 0.7057346  0.6479592  0.34681606 0.3991163 ]\n",
      "\n",
      "\n",
      "Epoch 500 Summary:\n",
      "Average D loss: 0.7008\n",
      "Average G loss: 0.6653\n",
      "Sample output: [0.41160372 0.43828732 0.49615043 0.60423535 0.40988135 0.7323148\n",
      " 0.564002   0.6501534  0.4513672  0.49987295]\n",
      "\n",
      "\n",
      "Epoch 510 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6659\n",
      "Sample output: [0.34788576 0.36568573 0.3455738  0.74468637 0.4557967  0.585244\n",
      " 0.67394435 0.66498345 0.57232773 0.355713  ]\n",
      "\n",
      "\n",
      "Epoch 520 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6672\n",
      "Sample output: [0.42545173 0.26809716 0.37305343 0.61056304 0.4144317  0.3967483\n",
      " 0.48786816 0.5868772  0.44373548 0.5170345 ]\n",
      "\n",
      "\n",
      "Epoch 530 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.21923296 0.43542513 0.27817553 0.5693053  0.43695286 0.45776936\n",
      " 0.62434393 0.5743591  0.6179564  0.42308122]\n",
      "\n",
      "\n",
      "Epoch 540 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6646\n",
      "Sample output: [0.3911908  0.53191423 0.22681536 0.5424899  0.47883758 0.5681814\n",
      " 0.39019737 0.65617305 0.5960783  0.51676506]\n",
      "\n",
      "\n",
      "Epoch 550 Summary:\n",
      "Average D loss: 0.7009\n",
      "Average G loss: 0.6650\n",
      "Sample output: [0.3125512  0.47153026 0.36574218 0.52267677 0.4283079  0.7185705\n",
      " 0.61942285 0.63991994 0.59172124 0.4099641 ]\n",
      "\n",
      "\n",
      "Epoch 560 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.36153793 0.56371856 0.44198164 0.37595725 0.54610026 0.47819105\n",
      " 0.43270448 0.5474593  0.5209865  0.36425984]\n",
      "\n",
      "\n",
      "Epoch 570 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6659\n",
      "Sample output: [0.3689605  0.44834286 0.28758806 0.5262707  0.61384606 0.5240959\n",
      " 0.5463935  0.6401744  0.5743758  0.51513696]\n",
      "\n",
      "\n",
      "Epoch 580 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6664\n",
      "Sample output: [0.41691887 0.46677464 0.20618482 0.4340335  0.5806503  0.64435184\n",
      " 0.34837484 0.5342971  0.52888846 0.4416775 ]\n",
      "\n",
      "\n",
      "Epoch 590 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.37754112 0.43001682 0.32441854 0.6153579  0.52427703 0.6201167\n",
      " 0.50163877 0.5686518  0.45450997 0.75916845]\n",
      "\n",
      "\n",
      "Epoch 600 Summary:\n",
      "Average D loss: 0.7005\n",
      "Average G loss: 0.6657\n",
      "Sample output: [0.4619071  0.2765154  0.30060884 0.39932075 0.53072685 0.5075456\n",
      " 0.5604525  0.74734336 0.3728646  0.62360954]\n",
      "\n",
      "\n",
      "Epoch 610 Summary:\n",
      "Average D loss: 0.7027\n",
      "Average G loss: 0.6656\n",
      "Sample output: [0.3162877  0.4722821  0.28317082 0.5892365  0.6173199  0.32404718\n",
      " 0.66108096 0.45579693 0.59982103 0.5641116 ]\n",
      "\n",
      "\n",
      "Epoch 620 Summary:\n",
      "Average D loss: 0.7007\n",
      "Average G loss: 0.6673\n",
      "Sample output: [0.5678092  0.4236983  0.28007928 0.5403095  0.47240824 0.34146065\n",
      " 0.5946146  0.57580334 0.5053002  0.43942702]\n",
      "\n",
      "\n",
      "Epoch 630 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.34097216 0.35912535 0.2541997  0.5597148  0.59348106 0.6162329\n",
      " 0.45285094 0.4903898  0.47333685 0.5170635 ]\n",
      "\n",
      "\n",
      "Epoch 640 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6646\n",
      "Sample output: [0.42441618 0.51149344 0.33698285 0.5412902  0.448044   0.5324331\n",
      " 0.5733839  0.575484   0.61252755 0.5235498 ]\n",
      "\n",
      "\n",
      "Epoch 650 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6647\n",
      "Sample output: [0.40047568 0.5193246  0.21010035 0.5426601  0.44272807 0.4581269\n",
      " 0.56439775 0.6429205  0.5898397  0.54962987]\n",
      "\n",
      "\n",
      "Epoch 660 Summary:\n",
      "Average D loss: 0.7006\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.46871546 0.19265112 0.34791175 0.56094545 0.42391413 0.29709446\n",
      " 0.43923867 0.7626325  0.40998924 0.40025905]\n",
      "\n",
      "\n",
      "Epoch 670 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6659\n",
      "Sample output: [0.2708092  0.39831167 0.29706195 0.4840114  0.5206043  0.58417135\n",
      " 0.4806129  0.66580755 0.5009335  0.5380433 ]\n",
      "\n",
      "\n",
      "Epoch 680 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6666\n",
      "Sample output: [0.47238505 0.47773015 0.25067842 0.630754   0.57073736 0.49695632\n",
      " 0.57507354 0.60398096 0.4638481  0.5323478 ]\n",
      "\n",
      "\n",
      "Epoch 690 Summary:\n",
      "Average D loss: 0.7007\n",
      "Average G loss: 0.6653\n",
      "Sample output: [0.50218797 0.4470609  0.27956668 0.43694308 0.52093285 0.5159159\n",
      " 0.49033788 0.6284818  0.6153068  0.66610265]\n",
      "\n",
      "\n",
      "Epoch 700 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6643\n",
      "Sample output: [0.41563004 0.5188619  0.36161506 0.39408275 0.44904932 0.6639895\n",
      " 0.47877908 0.6848856  0.50988793 0.44517052]\n",
      "\n",
      "\n",
      "Epoch 710 Summary:\n",
      "Average D loss: 0.7022\n",
      "Average G loss: 0.6648\n",
      "Sample output: [0.3334729  0.20816566 0.21324371 0.49466896 0.4933621  0.3467104\n",
      " 0.6052201  0.5294614  0.4809459  0.63246185]\n",
      "\n",
      "\n",
      "Epoch 720 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6664\n",
      "Sample output: [0.2808902  0.47055936 0.32350042 0.44399107 0.53915644 0.56508774\n",
      " 0.5571572  0.5079371  0.4892459  0.5772582 ]\n",
      "\n",
      "\n",
      "Epoch 730 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6654\n",
      "Sample output: [0.31036058 0.42250863 0.27322587 0.48406476 0.60450864 0.3668305\n",
      " 0.41606802 0.42519572 0.70376635 0.5507272 ]\n",
      "\n",
      "\n",
      "Epoch 740 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6654\n",
      "Sample output: [0.24500398 0.3583122  0.33605775 0.5345172  0.5171587  0.64384013\n",
      " 0.40742865 0.556933   0.4546732  0.37227988]\n",
      "\n",
      "\n",
      "Epoch 750 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6657\n",
      "Sample output: [0.31115887 0.42956302 0.18535979 0.63652426 0.41023073 0.57962847\n",
      " 0.575837   0.4761664  0.43731055 0.5452669 ]\n",
      "\n",
      "\n",
      "Epoch 760 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6650\n",
      "Sample output: [0.3502991  0.64689535 0.26122057 0.6506745  0.40979248 0.42755964\n",
      " 0.49444026 0.5650136  0.6931578  0.31764472]\n",
      "\n",
      "\n",
      "Epoch 770 Summary:\n",
      "Average D loss: 0.7024\n",
      "Average G loss: 0.6661\n",
      "Sample output: [0.48639548 0.24633965 0.30464053 0.5688214  0.44075084 0.6233402\n",
      " 0.4590542  0.70460844 0.53710616 0.2994168 ]\n",
      "\n",
      "\n",
      "Epoch 780 Summary:\n",
      "Average D loss: 0.7016\n",
      "Average G loss: 0.6661\n",
      "Sample output: [0.42063662 0.42177525 0.3292971  0.55854386 0.55540967 0.4790569\n",
      " 0.4661616  0.47197893 0.5631739  0.3652799 ]\n",
      "\n",
      "\n",
      "Epoch 790 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6667\n",
      "Sample output: [0.3280208  0.41269395 0.18054819 0.57865065 0.5779453  0.5339577\n",
      " 0.659965   0.54061234 0.5261407  0.5664382 ]\n",
      "\n",
      "\n",
      "Epoch 800 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6666\n",
      "Sample output: [0.45561558 0.47050864 0.33154964 0.4664759  0.52593786 0.6490026\n",
      " 0.5171138  0.6868599  0.49465868 0.4436059 ]\n",
      "\n",
      "\n",
      "Epoch 810 Summary:\n",
      "Average D loss: 0.7018\n",
      "Average G loss: 0.6667\n",
      "Sample output: [0.29766196 0.44949776 0.28262746 0.57093436 0.42498624 0.61408633\n",
      " 0.5948649  0.6814138  0.5632385  0.5765155 ]\n",
      "\n",
      "\n",
      "Epoch 820 Summary:\n",
      "Average D loss: 0.7007\n",
      "Average G loss: 0.6665\n",
      "Sample output: [0.27040145 0.2985762  0.4849254  0.5783512  0.5834743  0.46448734\n",
      " 0.3234538  0.558416   0.6359672  0.21964256]\n",
      "\n",
      "\n",
      "Epoch 830 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6662\n",
      "Sample output: [0.3043331  0.44997218 0.35342014 0.5214194  0.5520532  0.61886424\n",
      " 0.71450007 0.60830796 0.67691237 0.43854454]\n",
      "\n",
      "\n",
      "Epoch 840 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.4505702  0.6051394  0.46361187 0.55871856 0.6101821  0.4867889\n",
      " 0.56975293 0.66669136 0.60696316 0.40367702]\n",
      "\n",
      "\n",
      "Epoch 850 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6663\n",
      "Sample output: [0.27696452 0.39638063 0.38292068 0.63793    0.51328623 0.591228\n",
      " 0.50582486 0.5974331  0.50213283 0.41802904]\n",
      "\n",
      "\n",
      "Epoch 860 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6646\n",
      "Sample output: [0.3256304  0.40063623 0.33686665 0.6382058  0.6869135  0.5989468\n",
      " 0.51170313 0.6331384  0.47619537 0.33819655]\n",
      "\n",
      "\n",
      "Epoch 870 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6648\n",
      "Sample output: [0.44857475 0.3502702  0.30030215 0.59689105 0.52832544 0.4544662\n",
      " 0.43470073 0.7067459  0.51629007 0.48643738]\n",
      "\n",
      "\n",
      "Epoch 880 Summary:\n",
      "Average D loss: 0.7013\n",
      "Average G loss: 0.6643\n",
      "Sample output: [0.41065153 0.41403878 0.26532274 0.58669865 0.4030901  0.4506546\n",
      " 0.55617654 0.5589434  0.46636418 0.50720644]\n",
      "\n",
      "\n",
      "Epoch 890 Summary:\n",
      "Average D loss: 0.7002\n",
      "Average G loss: 0.6651\n",
      "Sample output: [0.38957545 0.2570125  0.24996841 0.6579483  0.56273896 0.3522924\n",
      " 0.5457766  0.49867588 0.42134252 0.4505386 ]\n",
      "\n",
      "\n",
      "Epoch 900 Summary:\n",
      "Average D loss: 0.7020\n",
      "Average G loss: 0.6671\n",
      "Sample output: [0.32444483 0.49878654 0.38497245 0.53553134 0.5064033  0.4799089\n",
      " 0.41766742 0.45132703 0.56963557 0.4903511 ]\n",
      "\n",
      "\n",
      "Epoch 910 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6677\n",
      "Sample output: [0.34869167 0.4401333  0.31151637 0.58411145 0.4120046  0.569598\n",
      " 0.49756303 0.53699654 0.4726354  0.37651348]\n",
      "\n",
      "\n",
      "Epoch 920 Summary:\n",
      "Average D loss: 0.7018\n",
      "Average G loss: 0.6656\n",
      "Sample output: [0.43657365 0.29286486 0.22365575 0.50633776 0.62256855 0.6112238\n",
      " 0.47698689 0.6814337  0.51343536 0.7695743 ]\n",
      "\n",
      "\n",
      "Epoch 930 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6682\n",
      "Sample output: [0.20357418 0.36482823 0.3604514  0.40806836 0.63752574 0.46048114\n",
      " 0.5465408  0.48370615 0.605085   0.6052559 ]\n",
      "\n",
      "\n",
      "Epoch 940 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6655\n",
      "Sample output: [0.26003277 0.32872242 0.38963965 0.56307876 0.3827987  0.41616097\n",
      " 0.54062057 0.45177233 0.4342763  0.43280655]\n",
      "\n",
      "\n",
      "Epoch 950 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6674\n",
      "Sample output: [0.24113499 0.4075294  0.31021392 0.5914111  0.45677844 0.49920505\n",
      " 0.553252   0.55518943 0.6266439  0.5267995 ]\n",
      "\n",
      "\n",
      "Epoch 960 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.21688153 0.41308406 0.3550285  0.5510291  0.59989697 0.570621\n",
      " 0.6507788  0.5164732  0.4553729  0.41504568]\n",
      "\n",
      "\n",
      "Epoch 970 Summary:\n",
      "Average D loss: 0.7014\n",
      "Average G loss: 0.6653\n",
      "Sample output: [0.38925523 0.5281441  0.34638327 0.6456864  0.57378924 0.42590916\n",
      " 0.4756698  0.35361847 0.6493742  0.5169127 ]\n",
      "\n",
      "\n",
      "Epoch 980 Summary:\n",
      "Average D loss: 0.7009\n",
      "Average G loss: 0.6656\n",
      "Sample output: [0.3648604  0.42546064 0.36470774 0.44472417 0.43667415 0.538128\n",
      " 0.70554286 0.5542862  0.49851686 0.58337647]\n",
      "\n",
      "\n",
      "Epoch 990 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.36294553 0.5509668  0.36861098 0.6179289  0.6045315  0.5310292\n",
      " 0.49473992 0.42845556 0.60373527 0.57508045]\n",
      "\n",
      "\n",
      "Epoch 1000 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6666\n",
      "Sample output: [0.46063343 0.5124514  0.31555316 0.40908018 0.40487877 0.4303694\n",
      " 0.5364775  0.49086735 0.474622   0.47516584]\n",
      "\n",
      "\n",
      "Epoch 1010 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6670\n",
      "Sample output: [0.39487    0.56300294 0.38530806 0.50074077 0.57699466 0.5962851\n",
      " 0.6431653  0.6636323  0.66771245 0.6702119 ]\n",
      "\n",
      "\n",
      "Epoch 1020 Summary:\n",
      "Average D loss: 0.7012\n",
      "Average G loss: 0.6645\n",
      "Sample output: [0.3180057  0.4578885  0.36504686 0.57340735 0.64103675 0.4262407\n",
      " 0.56261235 0.60252714 0.59586775 0.55217016]\n",
      "\n",
      "\n",
      "Epoch 1030 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.41744462 0.5611961  0.3656017  0.37077022 0.52925277 0.7405439\n",
      " 0.4992634  0.5379733  0.39245328 0.6229031 ]\n",
      "\n",
      "\n",
      "Epoch 1040 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6654\n",
      "Sample output: [0.34973374 0.48027292 0.33663502 0.4937597  0.4366271  0.5224681\n",
      " 0.5424222  0.688996   0.5827965  0.7049568 ]\n",
      "\n",
      "\n",
      "Epoch 1050 Summary:\n",
      "Average D loss: 0.7009\n",
      "Average G loss: 0.6673\n",
      "Sample output: [0.5034086  0.44839445 0.30242562 0.584018   0.4102481  0.5045227\n",
      " 0.5737262  0.64631385 0.4262732  0.483223  ]\n",
      "\n",
      "\n",
      "Epoch 1060 Summary:\n",
      "Average D loss: 0.7021\n",
      "Average G loss: 0.6661\n",
      "Sample output: [0.3976056  0.4901199  0.18313792 0.5320235  0.4409572  0.40194476\n",
      " 0.45740622 0.49110582 0.5738779  0.54394275]\n",
      "\n",
      "\n",
      "Epoch 1070 Summary:\n",
      "Average D loss: 0.7019\n",
      "Average G loss: 0.6662\n",
      "Sample output: [0.22158207 0.5621358  0.26677653 0.4988972  0.65599793 0.69328547\n",
      " 0.5111674  0.4743054  0.5600464  0.5572585 ]\n",
      "\n",
      "\n",
      "Epoch 1080 Summary:\n",
      "Average D loss: 0.7027\n",
      "Average G loss: 0.6657\n",
      "Sample output: [0.3221891  0.5632565  0.23603064 0.5202711  0.5811083  0.51168483\n",
      " 0.68713486 0.54157495 0.48341012 0.44267517]\n",
      "\n",
      "\n",
      "Epoch 1090 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6662\n",
      "Sample output: [0.2706369  0.31165344 0.35475796 0.56214184 0.56153435 0.44406244\n",
      " 0.4362072  0.557444   0.50382733 0.41878074]\n",
      "\n",
      "\n",
      "Epoch 1100 Summary:\n",
      "Average D loss: 0.7010\n",
      "Average G loss: 0.6673\n",
      "Sample output: [0.27771068 0.39942062 0.16551852 0.43063965 0.74485815 0.58437186\n",
      " 0.5130708  0.5002657  0.43386376 0.5505004 ]\n",
      "\n",
      "\n",
      "Epoch 1110 Summary:\n",
      "Average D loss: 0.7011\n",
      "Average G loss: 0.6660\n",
      "Sample output: [0.33795515 0.5819076  0.28810385 0.49578366 0.53781176 0.4731719\n",
      " 0.6283575  0.4443003  0.33159178 0.53082496]\n",
      "\n",
      "\n",
      "Epoch 1120 Summary:\n",
      "Average D loss: 0.7004\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.37019098 0.5686373  0.3414335  0.6539878  0.5424244  0.59632087\n",
      " 0.47799033 0.5045981  0.64088196 0.5690229 ]\n",
      "\n",
      "\n",
      "Epoch 1130 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6656\n",
      "Sample output: [0.25062996 0.5623682  0.3898571  0.6385304  0.59329295 0.6141124\n",
      " 0.57635665 0.48022777 0.48240644 0.47269824]\n",
      "\n",
      "\n",
      "Epoch 1140 Summary:\n",
      "Average D loss: 0.7017\n",
      "Average G loss: 0.6654\n",
      "Sample output: [0.38913107 0.28290886 0.31183887 0.573663   0.5171881  0.51203257\n",
      " 0.5158012  0.5619852  0.4705116  0.41490853]\n",
      "\n",
      "\n",
      "Epoch 1150 Summary:\n",
      "Average D loss: 0.7015\n",
      "Average G loss: 0.6665\n",
      "Sample output: [0.34070116 0.39257196 0.3958087  0.5733427  0.54005945 0.7560296\n",
      " 0.565548   0.6616586  0.5407348  0.5848101 ]\n",
      "\n",
      "\n",
      "Epoch 1160 Summary:\n",
      "Average D loss: 0.7022\n",
      "Average G loss: 0.6672\n",
      "Sample output: [0.2818313  0.39131856 0.30306804 0.5590399  0.40635717 0.5051985\n",
      " 0.40363503 0.60068566 0.4844151  0.3088491 ]\n",
      "\n",
      "\n",
      "Epoch 1170 Summary:\n",
      "Average D loss: 0.7006\n",
      "Average G loss: 0.6668\n",
      "Sample output: [0.45660597 0.473078   0.3438019  0.56190044 0.656188   0.45034713\n",
      " 0.49775657 0.63308376 0.4945011  0.42856058]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 105\u001b[0m\n\u001b[1;32m    101\u001b[0m d_fake \u001b[38;5;241m=\u001b[39m discriminator(fake_samples\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    102\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (adversarial_loss(d_real, real_labels) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m    103\u001b[0m          adversarial_loss(d_fake, fake_labels)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 105\u001b[0m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Train Generator\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Reshape tensor - flatten channel dimension\n",
    "X_tensor = X_tensor.squeeze(1)  # New shape: [1579, 2948]\n",
    "\n",
    "# Constants\n",
    "input_dim = 2948\n",
    "latent_dim = 100\n",
    "batch_size = 64\n",
    "hidden_dim = 512\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # Replace BatchNorm\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),  # Replace BatchNorm\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim * 2, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Setup training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Create dataset of malicious samples only\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "# Training loop\n",
    "\n",
    "\n",
    "# 1. Setup data\n",
    "malicious_mask = y_tensor[:, 1] == 1\n",
    "malicious_data = X_tensor[malicious_mask].squeeze(1)  # Remove channel dim\n",
    "dataset = data.TensorDataset(malicious_data)\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "# Training loop with improved error handling\n",
    "num_epochs = 10000\n",
    "max_grad_norm = 1.0\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "        batches = 0\n",
    "        \n",
    "        for i, (real_samples,) in enumerate(dataloader):\n",
    "            current_batch_size = real_samples.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_samples = real_samples.to(device)\n",
    "            real_labels = torch.ones(current_batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(current_batch_size, 1).to(device)\n",
    "            \n",
    "            z = torch.randn(current_batch_size, latent_dim).to(device)\n",
    "            fake_samples = generator(z)\n",
    "            \n",
    "            d_real = discriminator(real_samples)\n",
    "            d_fake = discriminator(fake_samples.detach())\n",
    "            d_loss = (adversarial_loss(d_real, real_labels) + \n",
    "                     adversarial_loss(d_fake, fake_labels)) / 2\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            g_output = discriminator(fake_samples)\n",
    "            g_loss = adversarial_loss(g_output, real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            total_d_loss += d_loss.item()\n",
    "            total_g_loss += g_loss.item()\n",
    "            batches += 1\n",
    "\n",
    "        # Generate samples\n",
    "        if epoch % 10 == 0:\n",
    "            generator.eval()  # Set to eval mode\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, latent_dim, device=device)  # Use full batch\n",
    "                fake = generator(z)\n",
    "                print(f\"\\nEpoch {epoch} Summary:\")\n",
    "                print(f\"Average D loss: {total_d_loss/batches:.4f}\")\n",
    "                print(f\"Average G loss: {total_g_loss/batches:.4f}\")\n",
    "                print(f\"Sample output: {fake[0][:10].cpu().numpy()}\\n\")\n",
    "            generator.train()  # Back to train mode\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loader\n",
    "Create a PyTorch DataLoader for efficient batch processing during GAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directories\n",
    "output_dir = \"gan_output\"\n",
    "model_dir = os.path.join(output_dir, \"models\")\n",
    "samples_dir = os.path.join(output_dir, \"samples\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "\n",
    "# Save function for training metrics\n",
    "def save_metrics(metrics, epoch):\n",
    "    metrics_file = os.path.join(output_dir, f\"metrics_epoch_{epoch}.json\")\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "# Save function for generated samples\n",
    "def save_samples(samples, epoch):\n",
    "    samples_file = os.path.join(samples_dir, f\"samples_epoch_{epoch}.npy\")\n",
    "    np.save(samples_file, samples.cpu().numpy())\n",
    "\n",
    "# Save function for model checkpoints\n",
    "def save_checkpoint(generator, discriminator, g_opt, d_opt, epoch):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state': generator.state_dict(),\n",
    "        'discriminator_state': discriminator.state_dict(),\n",
    "        'g_optimizer': g_opt.state_dict(),\n",
    "        'd_optimizer': d_opt.state_dict()\n",
    "    }\n",
    "    path = os.path.join(model_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "# Add to training loop:\n",
    "if epoch % 100 == 0:\n",
    "    # Save models\n",
    "    save_checkpoint(generator, discriminator, optimizer_G, optimizer_D, epoch)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'epoch': epoch,\n",
    "        'd_loss': total_d_loss/batches,\n",
    "        'g_loss': total_g_loss/batches\n",
    "    }\n",
    "    save_metrics(metrics, epoch)\n",
    "    \n",
    "    # Generate and save samples\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        fake_samples = generator(z)\n",
    "        save_samples(fake_samples, epoch)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
